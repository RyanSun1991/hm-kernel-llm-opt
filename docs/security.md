# Security

- Default to **internal LLM endpoints** only.
- Explicit allow-list for external proxy models.
- Add a redaction layer for prompts (paths, identifiers, proprietary strings).
- Store prompts/responses locally for auditability.
